%\documentclass[preprint]{aastex} %double-column, single-spaced document:
\documentclass[iop,floatfix]{emulateapj} 

\usepackage{hyperref}
%\usepackage{graphicx}
%\usepackage{apjfonts}
\usepackage{enumerate}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{bm}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\newcommand{\prob}{{\rm prob}}
\newcommand{\qN}{\{q_i\}_{i=1}^N}
\newcommand{\qM}{\{q_{im}\}_{i=1,m=0}^{N,M}}
\newcommand{\yN}{\{y_i\}_{i=1}^N}
\newcommand{\vt}{\vec{\theta}}
\newcommand{\vg}{\vt_{\star, {\rm grid}}}
\newcommand{\vpp}{\vt_{\star, {\rm post}}}
\newcommand{\vstar}{\vt_{\star}}
\newcommand{\finst}{f_{\lambda, {\rm inst}}}
\newcommand{\fsynth}{f_{\lambda, {\rm synth}}}
\newcommand{\vN}{\vt_{\rm N}}
\newcommand{\vc}{\vec{c}}
\newcommand{\fM}{ {\bm M}}
\newcommand{\fMi}{M_i}
\newcommand{\fD}{ {\bm D}}
\newcommand{\fDi}{D_i}
\newcommand{\fR}{ {\bm R}}
\newcommand{\dd}{\,{\rm d}}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\Z}{[{\rm Fe}/{\rm H}]}
\newcommand{\A}{[\alpha/{\rm Fe}]}

\newcommand{\todo}[1]{ \textcolor{Blue}{\\TODO: #1}}

%% Nomenclature guide
%%%%%%%%%%%%%%%%%%%%%
% * collections of synthetic spectra are generically called ``libraries.'' The term 
% ``grid'' is used when referring specifically to the spacing of spectral parameters
% in the library 


%\slugcomment{}
%\shorttitle{}
%\shortauthors{}

\begin{document}

\title{A Method for the spectroscopic inference of fundamental stellar parameters}
\author{\today{}\\
\medskip
Ian~Czekala\altaffilmark{1} et al.
%Author2\altaffilmark{2},
}

\altaffiltext{1}{Harvard-Smithsonian Center for Astrophysics, 60 Garden Street MS 10, Cambridge, MA 02138}
%\altaffiltext{2}{Institution 2}
\email{iczekala@cfa.harvard.edu}

\section{Introduction}
\begin{itemize}
  \item Where our technique fits in the ecosystem of stellar methods.
  \item Applicable to any kind of spectrum (long-slit, echelle, infrared, flux-calibrated or not)
  \item Examples of fields where accurate and unbiased stellar parameters are crucial. Exoplanets. T Tauri stars.
\end{itemize}

\section{Method}
Typically, the stellar properties we most desire are effective temperature $T_{\rm eff}$, radius or surface gravity $\log g$, and metallicity $\Z$. Several high-quality libraries of synthetic spectra now exist (PHOENIX, Kurucz, more being developed for GAIA) and are parameterized by these fundamental stellar parameters, which together, we call $\vg$. Synthetic libraries provide high-resolution spectra which span a range of $\vg$ covering the main sequence of spectral types. The library is typically specified on a grid of equal spacing in $T_{\rm eff}$, $\log g$, and $\Z$.

In addition to these fundamental parameters, a star also has several observed properties that are a function of its kinematics, geometry, and location in our galaxy: projected rotational velocity $v \sin i$, line of sight velocity $v_z$, total extinction $A_V$, and solid angle $\Omega$. We call these secondary parameters $\vpp$, because we can model their effects on the synthetic spectra in a ``post-processing'' step by convolution with an appropriate kernel or multiplication with a smooth function. Together, we call $\vstar = \{\vg, \vpp\}$.

Using a synthetic stellar library as a backend and applying post-processing techniques, we attempt to forward-model the observed stellar spectra in order to determine the best-fit $\vstar$. Because properly sampled spectra have correlated pixels, we present a framework which self-consistently models these correlations. 

\begin{table}[!htb]
\begin{tabular}{ll}
\hline
\hline
Symbol & Description\\
\hline
\hline
$i$ & index specifying a pixel\\
$\lambda_i$ & wavelength corresponding to a given pixel $i$\\
$\vg$ & fundamental stellar parameters, $T_{\rm eff}, \log(g), \Z, \A$\\
  & that parameterize a synthetic spectrum from the grid\\
$\vpp$ & stellar parameters $v \sin i$, $v_z$, $A_V$, and $R^2/d^2$ that\\
  & are applied during ``post processing'' of the synthetic spectrum\\
$\vstar$ & $\{\vg,\vpp \}$\\
$\finst(\lambda)$ & data spectrum\\
$\fsynth(\lambda)$ & synthetic spectrum\\
$c_0, c_1, \ldots$ & Chebyshev polynomial coefficients for residual flux calibration\\
$\vN$ & the set of nuisance parameters composed of $\{c_0, c_1, \ldots, c_N\}$\\
$k(i | \vN)$  & polynomial function that multiplies a synthetic spectrum\\
$\vt$ & the parameters $\{\vg, \vpp, \vN\}$ that completely describe a model spectrum\\
$\fDi$ & data flux for a given pixel, $D(\lambda_i)$\\
$\fD$ & data vector comprised of all $\fDi$, $i = \{0, \ldots, N\}$\\
$\fMi$ & model flux for a given pixel, $M(\lambda_i | \vt)$\\
$\fM$ & model vector comprised of all $\fMi$, $i = \{0, \ldots, N\}$\\
$\sigma_i$ & Poisson noise for a given pixel $i$\\
$R_i$ & residuals $\fDi - \fMi$\\
$\fR$ & residual vector $\fD - \fM$\\
\hline
\end{tabular}
\caption{Nomenclature used in this document}
\label{tab:nomenclature}
\end{table}

\subsection{Interpolation from synthetic spectral libraries}
Creating a new spectrum with a specific $\vg$ requires either synthesizing a new spectrum using radiative transfer codes or interpolating from nearby grid points in the library. Because it is computationally expensive to synthesize a spectrum at high resolution over a wide wavelength range, we choose to interpolate. Spline interpolation for spectra \citep{hus12}.
\todo{Implement band-limited interpolation from the synthetic grid using spline interpolation. Caution: As is, 100K/0.5 logg spacing in $\vg$ may not be Nyquist sampling.}
\todo{Investigate Tom Loredo's mentions of ``Cosmological Bayesian Emulator'' for using Gaussian processes for interpolating from a simulation grid.}

\subsection{Post-processing synthetic spectra to match the data}
Libraries of synthetic spectra are generally computed at high resolution ($R \geq $100,000), sampled with many pixels per resolution element, and do not include any rotational broadening or account for any instrumental effects. In order to transform a raw synthetic spectrum interpolated from the grid into one that matches the spectrum of a real star, we must post-process the spectrum to account for these secondary effects. The projected rotational velocity of the star, parameterized by $v \sin i$, broadens spectral line profiles and is mathematically described by convolution with a kernel $g_{v \sin i}(\lambda)$. UV, optical, and infrared spectra are acquired using a spectrograph which disperses light onto a CCD, defining a specific resolution and sampling rate for the spectrum. Because the resolution of the spectrograph and spacing of the pixels are different from the synthetic spectral library, we must convolve the raw spectrum with the line spread function (LSF) of the instrument $g_{\rm LSF}(\lambda)$ and resample to the exact pixels of the CCD. In wavelength space, the $v \sin i$ and LSF operations are represented by the convolution of the synthetic spectrum with these two kernels
\begin{equation}
  \finst(\lambda) = g_{v \sin i}(\lambda) \otimes g_{\rm LSF}(\lambda) \otimes \fsynth(\lambda)
  \label{eqn:convolution}
\end{equation}
Using the convolution theorem, we can rewrite these operations as multiplications in Fourier space 
\begin{equation}
  F_{\lambda, {\rm inst}}(s) = G_{\rm LSF}(s) G_{v \sin i}(s) F_{\lambda, {\rm synth}}(s)
\end{equation}
where $G \leftrightarrow g$ and $F \leftrightarrow f$ are Fourier transform pairs. In order to use the fast Fourier transform (FFT) to execute these operations using discrete samples of the synthetic spectrum $\fsynth$, we must first resample $\fsynth$ so that it is on a uniform grid. In the case of a spectrum, the natural uniform grid is one that is equally sampled in velocity space, such there is an equal velocity shift $\Delta v$ between pixels. This results in a wavelength grid that is linearly sampled in the logarithm of wavelength. Therefore, the Fourier coordinate $s$ is the number of cycles per sampling interval, having units of inverse velocity $[{\rm s}/{\rm km}]$. Next, we multiply the Fourier transform of the synthetic spectrum by the Fourier transforms of the rotational velocity and line spread function kernels. Lastly, we do an inverse FFT to transform the modified spectrum $F_{\lambda, {\rm inst}}$ back to wavelength space, where it is sampled at the wavelength locations of the pixels in the detector ($\finst$). When resampling the spectrum, we are careful to ensure band-limited interpolation using splines in order to prevent introducing non-physical high frequency structure into the spectrum.

Finally we apply corrections for interstellar extinction and Doppler shift. Interstellar extinction attenuates the spectrum by an amount $A_\lambda$, which is wavelength-dependent. We use the (name here) dust-extinction model, which is parameterized by extinction in the $V$ photometric band, $A_V$. We correct for a radial velocity difference $v_z$ between the star and the earth by Doppler shifting the model spectrum. 

\todo{Convolution in wavelength space is a possibility. Allows LSF to change with wavelength.}
Pixel width effects are not important as long as the LSF of the instrument is adequately sampled ($\gtrsim 3$ pixels across the LSF). If our spectrum were not properly sampled, we would need to additionally convolve with a boxcar the width of the pixels.

\todo{Doppler shifting the wavelengths could be implemented as a phase-shift in Fourier space, but it's pretty fast as-is.}

Due to small imperfections in the flux-calibration of the data spectrum, there may be broad regions of the spectrum which are offset from the true continuum level by a slight amount. A traditional approach to address this problem is to normalize both the data and the spectrum to the continuum level before comparison. While this may work well for hotter stars with well-defined continuua, for cooler stars with large molecular features, normalization may introduce artificial features into the spectrum when the pseudo-continuum is incorrectly placed. Instead, we choose to multiply the model spectrum by a low-order Chebyshev polynomial that is normalized to unity. The coefficients of the polynomial, $c_0, c_1, \ldots$, will be free-parameters in our model that we will solve for. If the behavior of the spectrograph is characterized by observations of spectrophotometric standards, then we can put priors on the coefficients that will prevent overfitting of the polynomials. 

With more uncertainty, the polynomial multiplication can also be used as a substitute for flux-calibration since the sensitivity function of an instrument is usually a smooth function with wavelength. When spectra are not flux calibrated, there is greater uncertainty about the properties of the spectrograph and wider priors must be used on the Chebyshev coefficients. In this case, the polynomial might destroy broad-scale information present in the spectrum by fitting it out, something that would be avoided with more accurate priors provided by the flux calibration for the model.

\subsection{Likelihood function}
To evaluate which parameters of the model $\fM$ fit the data set $\fD$ best we use a standard multidimensional Gaussian likelihood function which allows for covariance between data points. If the residuals vector is 
\begin{equation}
  \fR(\vstar) = \fD - \fM(\vstar)
\end{equation}
with $N$ data points, then the likelihood function is
\begin{equation}
  p(\fD | \vstar) = \frac{1}{\sqrt{(2 \pi)^N \det(C)}} \exp\left ( -\frac{1}{2} \fR^T C^{-1} \fR \right ) 
\end{equation}
and the log-likelihood function is
\begin{equation}
    \ln[p(\vstar)] = -\frac{1}{2} \fR^T C^{-1} \fR - \frac{1}{2} \ln \det C  - \frac{N}{2} \ln 2 \pi 
    \label{eqn:lnprob}
\end{equation}
$C$ is the covariance matrix which describes the covariance between pixels in the spectrum. For independent noise with noise per pixel $\sigma_i$, this matrix is
\begin{equation}
  C = 
  \begin{bmatrix}
    \sigma_0^2 & 0  & \cdots & 0\\
    0 & \sigma_1^2 & \cdots & 0\\
    \vdots  &   & \ddots  & \vdots \\
    0 & 0 & \cdots & \sigma_N^2\\
  \end{bmatrix}
\end{equation}
and Equation~\ref{eqn:lnprob} reduces to the familiar $\chi^2$ form of a sum over the square of the residuals, weighted by the inverse variance of each data point
\begin{equation}
  \ln[p(\vstar)] \propto - \frac{1}{2} \chi^2 = - \frac{1}{2} \sum_i \frac{R_i^2}{\sigma_i^2}
\end{equation}
However, residuals from a spectroscopic fit are often correlated due to the convolution of the stellar rotation and LSF kernels. Additionally, systematic errors in the synthetic spectra (such as incorrect line strengths) will resulted in regions of highly correlated residuals, which requires that we use a covariance matrix with off-diagonal terms $\sigma_{ij}$ and Equation~\ref{eqn:lnprob}, which includes a matrix product instead of a sum over independent pixels.

In order properly account for these correlated residuals, we must understand the covariance structure and parameterize it in our model. We seek to understand two types of covariance structure: the large scale global covariance structure that results from the stellar rotation and LSF kernels, and a regional covariance structure that is localized to areas where spectral line mismatch results in small patches of highly correlated residuals.

\todo{Tom Loredo was saying that if we have a model that perfectly fits the data then our residuals will be uncorrelated, regardless of the PSF/LSF. But is this true? Shouldn't all noise be correlated on some scale by the LSF? Correspond by email to clarify and show an example.}  

To account for the global covariance structure, we use a Matern 3/2 kernel.
\todo{Explore difference between using 3/2 Matern kernel and squared exponential kernel.}
\begin{equation}
  k_{\nu = 3/2}(r, a, l) = a \left(1 + \frac{\sqrt{3} r}{l} \right ) \exp \left (- \frac{\sqrt{3} r}{l} \right )
\end{equation}

tapered for compact support using a Hann window with width $r_0 = 6 l$ (Why? Because this looked good).

Specific line covariance. Non-stationary covariance kernel.
\begin{equation}
  k_{\rm G}(x, x^\prime | a, \mu, \sigma) = \frac{a^2}{2 \pi \sigma} \exp \left ( - \frac{[(x - \mu)^2 + (x^\prime - \mu)^2]}{2 \sigma^2}\right )
\end{equation}
\begin{equation}
k(x, x^\prime | h, a, \mu, \sigma) = \exp \left ( \frac{-( x - x^\prime)^2 }{2 h^2} \right ) k_{\rm G}(x, x^\prime | a, \mu, \sigma)
\end{equation}
$h$ is a "bandwidth." If $h$ is small, then there will be high-frequency structure. If $h$ is large, then only low-frequency structure will remain.

These are all parameters that we can sample in. Also, there would be a set of these parameters for each line in the model.

The benefit mainly comes from simply modelling these systematic residuals to begin with, since a model with covariance is far more likely than forcing the fit, and far more justified than masking regions which do not fit.

\subsection{Applications}
Learnt covariance structure can be used to correct the models.

\bibliography{disks,bayesian,master}
\bibliographystyle{hapj}
\end{document}
