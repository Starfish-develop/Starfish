\documentclass[preprint]{aastex} 
%\documentclass[iop,floatfix]{emulateapj} 

\usepackage[breaklinks,colorlinks,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{graphicx}
%\usepackage{apjfonts}
\usepackage{enumerate}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{color}
\usepackage[utf8]{inputenc}

%version-control tagging based off of github.com/bd-j/speccal
\input{vc}

\newcommand{\prob}{{\rm prob}}
\newcommand{\qN}{\{q_i\}_{i=1}^N}
\newcommand{\qM}{\{q_{im}\}_{i=1,m=0}^{N,M}}
\newcommand{\yN}{\{y_i\}_{i=1}^N}

\newcommand{\kms}{ \textrm{km s}^{-1} }

%\vt stands for ``vector theta''
\newcommand{\vt}{ {\bm \theta}}
\newcommand{\vT}{ {\bm \Theta}}
\newcommand{\vg}{\vt_{\star, {\rm grid}}}
\newcommand{\vpp}{\vt_{\star, {\rm post}}}
\newcommand{\finst}{f_{\lambda, {\rm inst}}}
\newcommand{\fsynth}{f_{\lambda, {\rm synth}}}
\newcommand{\vN}{\vt_{\rm N}}
\newcommand{\vtstar}{\vt_{\star}}
\newcommand{\vtcheb}{\vt_{\rm Cheb}}
\newcommand{\vtglobal}{\vt_{\rm global}}
\newcommand{\vtorder}[1]{\vt_{{\rm order}_{#1}}}
\newcommand{\vtorders}{\vt_{\rm orders}}
\newcommand{\vtline}[1]{\vt_{{\rm line}_{#1}}}
\newcommand{\vtlines}{\vt_{\rm lines}}
\newcommand{\vtcov}{\vt_{\rm cov}}
\newcommand{\vM}{\mathsf{M}}
\newcommand{\vD}{\mathsf{D}}
\newcommand{\vR}{\mathsf{R}}
\newcommand{\vC}{\mathsf{C}}
\newcommand{\fM}{ \vec{{\bm M}}}
\newcommand{\fMi}{M_i}
\newcommand{\fD}{ \vec{{\bm D}}}
\newcommand{\fDi}{D_i}
\newcommand{\fR}{ {\bm R}}
\newcommand{\dd}{\,{\rm d}}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\Z}{[{\rm Fe}/{\rm H}]}
\newcommand{\A}{[\alpha/{\rm Fe}]}
\newcommand{\matern}{Mat\'{e}rn}

\newcommand{\todo}[1]{ \textcolor{blue}{\\TODO: #1}}
\newcommand{\comm}[1]{ \textcolor{red}{SA: #1}}

\shorttitle{Spectroscopic inference}
\shortauthors{Czekala et al.}

\begin{document}

\graphicspath{{figs/}}

\slugcomment{draft: \today{}}


\title{A Method for the Spectroscopic Inference of Stellar Parameters}
\author{Ian Czekala, Sean M.~Andrews, et al.}
\affil{Harvard-Smithsonian Center for Astrophysics, 60 Garden Street, Cambridge, MA 02138}
%\email{iczekala@cfa.harvard.edu}

\section{Introduction} \label{sec:intro}

All astronomers recognize that spectroscopy offers a wealth of information that can help
characterize the properties of the observing target.  In the context of stellar astrophysics, 
spectroscopy plays many fundamental roles.  The relative strengths and widths of stellar absorption 
lines provide access to key parameters like effective temperature ($T_{\rm eff}$) and surface 
gravity ($\log g$), enabling model comparisons in the Hertzsprung-Russell diagram to estimate the 
masses and ages so crucial to understanding stellar evolution, as well as individual elemental (and 
molecular) abundances or the collective ``metallicity" (typically parameterized as [Fe/H]), 
facilitating study of the chemical hallmarks of different stellar populations.  With sufficient 
spectral resolution, the velocity content of a spectrum can convey crucial information about 
stellar rotation ($v \sin i$) and kinematics (e.g., association with a cluster or companion through 
the radial velocity, $v_z$).  While many fields benefit from the spectroscopic measurements of 
these stellar properties, there is acute interest in the exoplanet community.  There, all estimates 
of the planet properties are made {\it relative} to the host properties (e.g., the planet-to-host 
mass or radius {\it ratio} is measured with the radial velocity or transit techniques, 
respectively).  Moreover, essential clues to the planet formation process are encapsulated in the 
dependences of planet frequency on host mass \citep[e.g.,][]{johnson07,howard10} and metallicity 
\citep[e.g.,][]{fischer05,buchhave14}.   
% SA: we ought to check with exoplanet people on these references. Can do later...

That said, the robust and quantitative extraction of physical (or empirical) parameters from an 
observed spectrum can be an extraordinary challenge.  Stellar models are ultimately required as a 
comparative benchmark to associate observed spectral features with the parameters of interest.  
Generating a synthetic model spectrum requires a complex numerical treatment of the stellar 
structure and radiative transfer through the atmosphere \citep[e.g.,][]{kurucz93,castelli04,
hauschildt99,husser13,paxton11}.  Detailed models calibrated to individual stars are important, but 
rare (e.g., the Sun, Vega); as such, these stellar models are relatively untested in large swaths 
of parameter-space.  Moreover, they necessarily include simplifications to treat complicated 
physical processes (e.g., convection) or computational limitations (e.g., geometry, boundary 
conditions), and often must rely on incomplete or inaccurate atomic and molecular information 
(e.g., oscillator strengths, opacities).  In principle, the models could be further improved with 
appropriate reference to spectroscopic datasets.  Nevertheless, they achieve a remarkable level of 
success in reproducing many key diagnostic features in stellar spectra.  

There are various well-tested techniques being used to compare these models with observed spectra 
and thereby infer basic stellar parameters; here we highlight three examples to illustrate some key 
ideas.  First is a straightforward empirical approach that relies on distilling an information-rich 
subset of the spectral data, usually in the form of spectral line equivalent widths and/or local 
continuum shapes.  A combined sequence of the ratios of these quantities are found to be especially 
sensitive to one key parameter in the models, and therefore can be used to quantify it 
\citep[e.g.,][]{gray94,reid95,rojas-ayala10,rojas-ayala12}.  The line index comparison approach has 
the major advantage of being trivially fast, but each index relationship is only informative over 
a limited range of parameter-space.  A second technique exploits the cross-correlation of an 
observed spectrum with a suite of model templates to identify an optimized set of parameters, 
usually with some preferential weighting applied to specific spectral regions \citep[e.g., {\tt 
SPC};][]{buchhave12}.  In this case, the speed advantage is maintained while taking advantage of 
more data content (particularly in the spectral dimension), thereby achieving high precision even 
for data with comparatively low sensitivity; the disadvantage is that the model quality and 
parameter inferences are assessed in an empirical, rather than probabilistic, framework.  The third 
approach of interest prefers a direct, pixel-by-pixel comparison between model and observed 
spectra, and is usually associated with a spectral synthesis front-end (e.g., {\tt MOOG}; 
\citealt{sneden73}; {\tt SME}; \citealt{valenti96}).  This technique has the benefits of parametric 
flexibility (e.g., one can fit for atomic abundances or arbitrary structures) and a proper 
inference framework \citep[usually a least-squares approach, although increasingly in a Bayesian 
format;][]{shkedy07,schoenrich13}, but it is computationally expensive and can be biased by model 
systematics \citep[e.g.,][]{mann13}; in practice, this often requires an effort focused on a 
pre-selected subsample of the data. 

In this article, we design a flexible forward-modeling approach to the general spectroscopic 
inference problem in a Bayesian framework, building on the best aspects of the latter two methods 
highlighted above.  Employing a non-trivial covariance matrix parameterized by both global 
(stationary) and local (non-stationary) Gaussian process kernels, we are able to account for 
residual pixel-by-pixel correlations that arise when comparing observed spectra to intrinsically 
imperfect models.  This approach minimizes the potential for bias, efficiently propagates 
systematic uncertainties into the parameter inferences, and ultimately can serve as tool for using
observations to guide substantially improvements in the models themselves.  An overview of the 
methodology of this approach is provided in Section \ref{sec:method}, including the covariance 
matrix formalism.  Some relevant tests and example applications (specifically for a high resolution 
optical spectrum of an F star, and a medium-resolution near-infrared spectrum of a mid-M star) of 
the method are described in Section \ref{sec:examples}.  Finally, a discussion of the potential 
utility of the technique, and the possibility of extending it to develop data-driven spectral 
models, is provided in Section \ref{sec:discussion}.  


\section{Methodology} \label{sec:method}

Here we describe a generative Bayesian modeling framework that confronts some of the key obstacles 
in the spectroscopic inference problem.  The goal of this approach is to conservatively extract the 
maximal amount of information about a prescribed (and degenerate) parameter set by forward-modeling 
an observed spectrum, while also recognizing and explicitly accounting for the covariances and 
biases introduced by pathologically imperfect models or calibrations.  The method is modular, and 
therefore can easily incorporate additional physical or nuisance parameters as desired without 
sacrificing an accurate reflection of the limitations in the data.  Moreover, with a well-crafted 
observational sample, this data-driven approach should ultimately enable us to systematically learn 
how synthetic spectral models can be improved.  The specific applications discussed here are 
related to the spectra of individual stars, but the methodology is generic (and could be used for 
composite spectra of unresolved stellar clusters, galaxies, etc.).  

The remainder of this section describes the mechanics of this modeling framework.  First, a model 
spectrum is generated for a given set of physical parameters (Section \ref{subsec:synthetic}), and 
then post-processed to mimic reality using a set of observational and practical nuisance parameters 
(Section \ref{subsec:postprocess}).  Next, a direct, pixel-by-pixel comparison between the data and 
model spectra is made with a prescribed likelihood function and a parametric treatment of the 
covariances between pixel residuals (Section \ref{subsec:likelihood}).  That process is iterated 
under the guise of hierarchical Monte Carlo Markov Chain (MCMC) simulations to numerically explore 
the posterior probability density of the model conditioned on the data, and thereby to determine 
constraints on the parameters of interest (Section \ref{subsec:MCMC}).  Along the way, these 
procedures are illustrated with real observations of the high resolution optical spectrum from a 
nearby F star.  That specific application, along with some alternative demonstrations of the 
method, are discussed in more detail in Section \ref{sec:examples}.


\subsection{Generating a Model Spectrum \label{subsec:synthetic}}

There are various approaches to synthesizing a spectrum, $f_{\lambda}$, for a specific set of model
parameters, $\vt_{\ast}$.  In an ideal case, a model stellar atmosphere is constructed and then 
subsequently processed through a radiative transfer code \citep[e.g.,][]{kurucz93,hauschildt99}.  
However, in general this approach is still computationally prohibitive for any iterative method of 
probabilistic inference.  One partial compromise is to interpolate over a library of atmosphere 
structures that were pre-computed for a discrete grid of parameter values, $\{\vt_{\ast}\}^{\rm 
grid}$, for some arbitrary $\vt_{\ast}$, and then perform a radiative transfer calculation with 
that interpolated atmosphere to synthesize $f_{\lambda}$ \citep[e.g., as for {\tt 
SME};][]{valenti96}.  A more common variant is to instead rely on interpolation over a library of 
pre-synthesized model spectra, $f_{\lambda}(\{\vt_{\ast}\}^{\rm grid})$ \citep[e.g.,][]{castelli04,
allard12,husser13}.  While technically the former approach is most similar to the ideal case, the 
computational cost of repeated spectral synthesis is sufficiently high to make a detailed 
exploration of parameter space (particularly for data with a large spectral range) considerably 
less appealing.  A related, but different approach is to eschew forward modeling entirely (and 
therefore repeated spectral syntheses and/or library interpolations), and instead evaluate the 
models only at the discrete grid points of the library.  Then, these discretized samples of the 
posterior probability density can be interpolated to an arbitrary $\vt_{\ast}$ to construct 
appropriate confidence intervals \citep[similar to the method of {\tt SPC};][]{buchhave12}.  The 
difficulty with this latter approach is that the parameter uncertainties can be smaller than the 
grid spacing; in that case, there is valid concern that this interpolation might not accurately 
recover intrinsic parameter degeneracies.

Here we opt to take the computationally expedient approach that employs a library of model spectra, 
$f_{\lambda}(\{\vt_{\ast}\}^{\rm grid})$, where $\vt_{\ast} = [T_{\rm eff}, \,\, \log{g}, \,\, Z]$ 
(in practice, the metallicity $Z$ is often parameterized by [Fe/H]).  However, it is worth noting 
that the techniques we will develop are applicable to {\it any} reasonable ``front-end" that 
generates a model spectrum.  In our adopted approach, the model spectrum for an arbitrary 
$\vt_{\ast}$ must be interpolated from among the spectral library, 
\begin{equation}
f_{\lambda}(\{\vt_{\ast}\}^{\rm grid}) \leadsto f_{\lambda}(\vt_{\ast}),
\label{eqn:interp}
\end{equation}
where we assign the symbol $\leadsto$ as an interpolation operator.  The multi-dimensional 
interpolation in Eq.~\ref{eqn:interp} needs to be performed many times, so computational efficiency 
is critical.  In practice, a simple tri-linear interpolation is suitably fast, but introduces an 
undesireable level of inaccuracy (particularly in the $Z$ dimension).  The interpolation quality 
can be empirically estimated by performing the operation in Eq.~\ref{eqn:interp} across a 
calculated location in $\{\vt_{\ast}\}^{\rm grid}$, and then comparing the interpolated spectrum 
with the corresponding library spectrum.  After an extensive exploration of such calculations 
\citep[see also][]{husser12}, we concluded that the best combination of speed and accuracy can be 
achieved by pre-computing a {\it refined} spectral library using a cubic spline interpolation with 
a $\{\vt_{\ast}\}^{\rm grid}$ spacing of [20\,K, 0.1\,dex, 0.1\,dex], and then performing 
tri-linear interpolation over that refined grid.  Overall, this interpolation technique is found to 
be accurate within a few percent per high resolution model pixel.  Ideally, this pre-interpolation 
could be avoided if the spectral library was computed over a refined grid (with a substantial 
up-front computational investment); but for the time being, we can empirically propagate these 
interpolation uncertainties into the likelihood calculations (as will be described in 
Section~\ref{subsec:likelihood}).  
\comm{I see nothing was done here regarding your updated approach; how should we handle that?}


\subsection{Post-Processing} \label{subsec:postprocess}

Generally, the ``raw" model spectrum $f_{\lambda}(\vt_{\ast})$ is highly over-sampled compared to a 
typical observed spectrum, and does not account for several additional observational and 
instrumental effects that become important in comparisons with real data.  Therefore, a certain 
amount of post-processing is required before assessing the model quality.  We treat that 
post-processing in two stages: the first deals with an additional set of ``observational" 
parameters, $\vt_{\rm obs}$, that incorporate dynamical effects, geometry, and the relative 
location of the target, while the second employs a suite of nuisance (hyper-)parameters, $\vT_{\rm 
n}$, designed to mitigate an imperfect data calibration.

We can further divide $\vt_{\rm obs}$ into those parameters that impact the model primarily in the 
spectral or flux dimensions; $\vt_{\rm obs} = [\vt_{{\rm obs,} v}, \,\, \vt_{{\rm obs,} f}]$.  For 
the former, we consider three kernels that contribute to the observed line-of-sight velocity 
distribution function, $\varphi_v$.  The first, $\mathcal{F}_v^{\rm inst}$, treats the instrumental 
spectral broadening.  For illustrative purposes we assume $\mathcal{F}_v^{\rm inst}$ is a Gaussian 
with a mean at $v = 0$ and a constant width $\sigma_v$ at all $\lambda$, although more 
sophisticated forms could be adopted.  The second, $\mathcal{F}_v^{\rm rot}$, characterizes the 
broadening induced by (projected) stellar rotation, parameterized by $v\sin{i}$ as described by 
\citet[][his Eq.~18.14]{gray08}.  And the third, $\mathcal{F}_v^{\rm dop} = \delta(v-v_z)$, 
incorporates the radial velocity through a Doppler shift.  The model spectrum is modified by the 
parameters $\vt_{{\rm obs}, v} = [\sigma_v, \,\, v\sin{i}, \,\, v_z]$ through these kernels, using 
a convolution in velocity-space,\footnote{In practice, these convolutions are performed as 
multiplications in Fourier-space to better preserve spectral information \citep[cf.,][]{tonry79}; 
the mathematical formalism is presented here for clarity.}
\begin{equation}
f_{\lambda}(\vt_{\ast}, \vt_{{\rm obs}, v}) = f_{\lambda}(\vt_{\ast}) \otimes \varphi_v = f_{\lambda}(\vt_{\ast}) \otimes \mathcal{F}_v^{\rm inst} \otimes \mathcal{F}_v^{\rm rot} \otimes \mathcal{F}_v^{\rm dop},
\label{eqn:broadening}
\end{equation}
and then re-sampled onto the discrete wavelengths corresponding to each data pixel, 
\begin{equation}
f_{\lambda}(\vt_{\ast}, \vt_{{\rm obs}, v}) \mapsto \vM(\vt_{\ast}, \vt_{{\rm obs}, v}),
\label{eqn:resampling}
\end{equation}
where the $\mapsto$ symbol denotes a re-sampling operator that maps the model spectrum onto the 
$N_{\rm pix}$-dimensional array $\vM$ (and $N_{\rm pix}$ is the number of pixels in the spectrum).  
For reference, Figure \ref{fig:broadening} shows a graphical representation of these 
post-processing steps.

\begin{figure}[!t]
\begin{center}
  \includegraphics{figs/kernels.pdf}
  \includegraphics{figs/high2low.pdf}
  \caption{Panel 1: the instrumental (${\rm FWHM} =6.8\;\kms$) and rotational ($v \sin i = 5\;\kms$) broadening kernels, and their composite. Panel 2: A section of the raw, high resolution synthetic model at $R=$500,000. Panel 3: The spectrum after applying the composite broadening kernel, $\varphi_v$. Panel 4: The broadened spectrum downsampled to the pixels of the detector.}
\label{fig:broadening}
\end{center}
\end{figure}

At this stage, the model is further modified in the flux dimension.  A typical synthetic spectrum 
is computed as the flux that would be measured {\it at the stellar surface}, and so needs to be 
diluted by the subtended solid angle, $\Omega = (R_{\ast}/d)^2$, where $R_{\ast}$ is the stellar 
radius and $d$ is the distance.  An additional wavelength-dependent scaling factor is applied to 
account for interstellar extinction, assuming some previously-derived extinction law $A_{\lambda}$ 
\citep[e.g.,][]{cardelli89} that is parameterized by $A_V$.  The parameters $\vt_{{\rm obs}, f} = 
[\Omega, \,\, A_V]$ are applied as
\begin{equation}
\vM(\vt) \equiv \vM(\vt_{\ast}, \vt_{\rm obs}) = \vM(\vt_{\ast}, \vt_{{\rm obs}, v}) \times \Omega \times 10^{-0.4\,A_{\lambda}},
\label{eqn:scaling}
\end{equation}
where we have simplified the notation by writing $\vt \equiv [\vt_{\ast}, \,\, \vt_{\rm obs}]$.

So far, the procedure summarized in Eq.~\ref{eqn:interp}-\ref{eqn:scaling} is composed of 
relatively straightforward operations demanded by practical astronomical and computing issues.  If 
the observed spectrum was {\it perfectly} calibrated, we could proceed to a likelihood calculation 
(Section~\ref{subsec:likelihood}) that makes a direct comparison with $\vM(\vt)$ at this point.  
However, that is unlikely to be the case.  The primary concern is that an imperfect calibration 
produces (presumably low-level) mismatches in the underlying shape of the observed spectrum on 
relatively broad wavelength scales.  When compared with the model, these mismatches might represent 
a non-trivial contribution to the likelihood, and thereby bias our estimates of the desired 
physical parameters.  Often, this concern is treated externally to any modeling procedure, usually 
by dividing the observed spectrum (and model spectrum) by an appropriate polynomial function.  But 
that ``normalization" procedure implicitly assumes that there is no relevant information content on 
those mismatched scales; if $\vt$ also contributes to the broad spectral shape, then adopting this 
approach will corrupt the inferences of these parameters.  Moreover, in practice this approach is 
limited, since defining an appropriate polynomial becomes difficult in cases where the spectral 
line density is high (e.g., molecular bands for cool stars).  

We adopt a somewhat analogous approach to deal with this issue, but cast it {\it internal} to the 
modeling framework to appropriately propagate the uncertainty introduced by additional degrees of 
freedom in the model.  The residual calibration errors are treated as an explicit contribution to 
the model spectrum, enabling us to explore the distribution of possible calibrations (see 
Section~\ref{subsec:MCMC}) and then eventually marginalize out the associated nuisance parameters.  
In essence, the inferences for the relevant stellar parameters will properly account for the 
underlying uncertainty in the calibration process.  This is achieved in practice by distorting the 
model spectrum with a (low-order) Chebyshev polynomial \citep[e.g.,][]{eisenstein06,koleva09}, 
\begin{equation}
\vM(\vt, \vT_{\rm cheb}) = \vM(\vt) \times \sum_n c_n \, T_n,
\label{eqn:chebyshev}
\end{equation}
where $T_n$ are the standard Chebyshev functions of order $n$ and the coefficients are treated as a
set of nuisance (hyper-)parameters, $\vT_{\rm cheb} = \{c_n\}^{\rm order}$, for each available 
spectral order in the dataset.  With judicious priors on $\vT_{\rm cheb}$, we can ensure that the 
unintended treatment of real spectral features (e.g., broad, deep molecular bands) as calibration 
artifacts is negligible (see Section~\ref{sec:examples} for examples).  The lowest-order (scaling) 
coefficient, $c_0$, is by its nature degenerate with the solid angle parameter, $\Omega$.  
Therefore, we enforce an additional constraint by requiring that the mean of the polynomial is 
unity.  For data with a single spectral order, this means setting $c_0 = 1$.  If the goal is to 
model multiple spectral orders, we assign $c_0 = 1$ in an arbitrary order as an anchor, but permit 
the $c_0$ values in other orders to be different (as necessary).  It is worth noting that this 
formalism can, in principle, be extended to develop models for completely uncalibrated spectra, 
rather than the residual calibration mismatches as described here (with a suitable relaxation of 
the priors on $\vT_{\rm cheb}$).  Figure \ref{fig:chebyshev} offers a practical demonstration of 
how these nuisance parameters are applied. 

\begin{figure}[!htb]
\begin{center}
  \includegraphics{figs/chebyshev.pdf}
  \caption{The spectrum at the overlap of two echelle orders (23 \& 24). \textbf{Panel 1}: The flux-calibrated dataset shows a slight discrepancy of $\lesssim 3$\% between orders. \textbf{Panel 2}: To account for this residual error in the flux calibration, we multiply the model spectrum by a Chebyshev polynomial, whose coefficients are parameters that are part of the model. \textbf{Panels 3 \& 4}: In principle, we could divide the data by these polynomials to recover what the true flux-calibrated data should be (panel 3), but instead we multiply the model by the polynomials to recover the original discrepancy between orders (panel 4). This has the advantage of preserving the original dataset as a fixed quantity and makes the model (Equation~\ref{eqn:chebyshev}) linear in the Chebyshev polynomial coefficients.}
\label{fig:chebyshev}
\end{center}
\end{figure}


\subsection{Model Evaluation} \label{subsec:likelihood}

The quality of the model spectrum is assessed by comparing to the data with a pixel-by-pixel 
likelihood calculation.  If we denote the data spectrum as $\vD$, then a corresponding residual 
spectrum (of $N_{\rm pix}$ pixels) can be defined for any input parameter set,
\begin{equation}
\vR \equiv \vR(\vt, \vT_{\rm cheb}) \equiv \vD-\vM(\vt, \vT_{\rm cheb}).
\end{equation}
To quantify the probability of the data conditioned on the model, we adopt a standard 
multi-dimensional Gaussian likelihood function,
\begin{equation}
p(\vD|\vM) =  \frac{1}{[(2 \pi)^{N_{\rm pix}} \det(\vC)]^{1/2}} \exp\left ( -\frac{1}{2} \,
   \vR^\trans \vC^{-1} \vR \right ), 
   \label{eqn:prob}
\end{equation}
which penalizes models that yield larger residuals and explicitly allows for covariances in the 
residual spectrum through the $N_{\rm pix} \times N_{\rm pix}$ matrix, $\vC$.  For practical 
numerical reasons, we generally consider the log-likelihood as the fit quality metric, which in 
this case is
\begin{equation}
  \ln{p(\vD | \vM)} = -\frac{1}{2} \left( \vR^\trans \vC^{-1} \vR + \ln{\det{\vC}} + N_{\rm pix} \ln{2\pi} \right).
  \label{eqn:lnprob}
\end{equation}

The covariance matrix $\vC$ characterizes both the measurement uncertainty ($\sigma$; ``noise") in 
each pixel and the intrinsic covariance between pixels.  The special case where each pixel 
represents an independent measurement results in a diagonal covariance matrix, $\vC_{ij} = 
\delta_{ij} \,\, \sigma_i$ where $\sigma_i$ is the uncertainty in pixel $i$ and $\delta_{ij}$ is 
the Kronecker delta function, and Eq.~\ref{eqn:lnprob} reduces to the familiar
\begin{equation}
\ln{p(\vD | \vM)} = -\frac{1}{2} \sum_i^{N_{\rm pix}} \frac{\vR_i^2}{\sigma_i^2} \equiv -\frac{\chi^2}{2},
\label{eqn:chisq}
\end{equation}
the sum of the square of the residuals weighted by the inverse variances (squared uncertainties).  
However, the problem being addressed here necessitates the use of a more complex covariance matrix; 
additional off-diagonal terms that can explicitly characterize (1) pixel-to-pixel covariances 
imposed by the discrete over-sampling of the line-spread function, and (2) highly correlated 
residuals as manifestations of the still-imperfect model library are required to avoid biasing our 
inferences of the physically interesting parameters ($\vt$).  The following subsections describe 
how these issues are addressed in the practical implementation of $\vC$.  


\subsubsection{Global Covariance Structure} \label{subsec:global_covariance}

Astronomical spectrographs are designed so that the detector over-samples the instrumental line 
broadening function with at least a few pixels.  Therefore, adjacent pixels are never completely 
independent samples of the observed spectrum.  In that case, a difference between an observed and 
modeled spectral feature will create a residual that spans multiple pixels.  This can be 
demonstrated in practice by examining the autocorrelation of the residual spectrum: a slight model 
mismatch will produce correlated residuals over a characteristic scale similar to the observational 
line-broadening kernel width.  Figure \ref{fig:class0} highlights a specific example of these 
correlated residuals in real data, where an imperfect model generates residuals which exhibit a 
significant autocorrelation signal on the scale of $\sim$4 pixels, which corresponds to the typical 
line width in this spectrum.  

\begin{figure*}[!htb]
\begin{center}
  \includegraphics{correlation_panel.pdf}
  \caption{\textbf{Top}: the big spectrum. \textbf{Left} The same low-amplitude, mildly covariant residuals from Figure~\ref{fig:badlines}, panel 0, but enlarged to show the mildly covariant structure produced by slight mismatch between the data and model spectra.   
  \textbf{Right} The autocorrelation of the residual sequence shown at left. Notice that there is significant correlation for offsets of $\lesssim 4$ pixels.}
\label{fig:class0}
\end{center}
\end{figure*}

It seems important to distinguish here between ``noise" and the fit residuals.  Noise introduced to 
the spectrograph by astrophysical or instrumental effects is generally uncorrelated with 
wavelength.  The arrival of each photon to the detector is an independent event; while these 
photons are scattered by the instrumental line-spread function, the magnitude and direction of that 
scatter is independent for each such event.  In essence, the noise itself is uncorrelated, but the 
fit residuals likely are correlated.  However, from a mathematical perspective the correlated 
residuals can be treated in the same way as correlated noise, by constructing a non-trivial 
covariance matrix with off-diagonal terms.  In practice, this is achieved by parameterizing $\vC$ 
with a kernel that describes the covariance between any pair of pixels, representing wavelengths 
$\lambda_i$ and $\lambda_j$ (in many ways analogous to the standard two-point spatial correlation 
function used in cosmology).

For a well-designed spectrograph and sufficiently accurate model, this {\it global} (i.e., present 
throughout the spectrum) covariant structure should have a relatively low amplitude and small 
correlation length.  To describe that structure, we use a stationary covariance kernel (or radial 
basis function) with an amplitude that depends only on the velocity distance between two pixels, 
\begin{equation}
  r_{ij} = r(\lambda_i, \lambda_j) = \Delta v = \frac{c}{2} \left | \frac{\lambda_i 
   - \lambda_j}{ \lambda_i + \lambda_j} \right |,
\end{equation}
where $c$ is the speed of light.  The kernel is chosen to parametrically describe the covariance 
between pixel residuals, such that
\begin{equation}
  \mathcal{K}_{\rm g}(\lambda_i, \lambda_j) =  \langle \vR_i \; \vR_j \rangle.
  \label{eqn:expectation}
\end{equation}
A variety of kernels have been used in the field of Gaussian processes to parameterize such a 
mildly covariant structure \citep[e.g.,][]{rasmussen05}.  We adopt the \matern\ kernel with $\nu = 
3/2$,
\begin{equation}
  \mathcal{K}_{\rm g}(\lambda_i, \lambda_j \, |\, a_{\rm g},\, \ell) = a_{\rm g} \left(1 + \frac{\sqrt{3}\, r_{ij}}{\ell} \right ) \exp 
   \left (- \frac{\sqrt{3}\, r_{ij}}{\ell} \right ),
   \label{eqn:global}
\end{equation}
which is parameterized by an amplitude $a_g$ and scale $\ell$ and makes for a smooth transition to 
negligible covariance at large $r$.  To ensure that $\vC$ remains a relatively sparse matrix that 
enables computational expendiency, we employ a Hann window function
\begin{equation}
  w\,(\lambda_i, \lambda_j |\, r_0) = \left \{ 
    \begin{array}{cc}
    \frac{1}{2} + \frac{1}{2} \cos \left(\frac{\pi r_{ij}}{r_0} \right) & r_{ij} \le r_0 \\
    0 & r_{ij} > r_0 \\
  \end{array}
  \right .
  \label{eqn:Hann}
\end{equation}
to taper the kernel (so the effective kernel is $w \, \mathcal{K}_g$).  The truncation distance 
$r_0$ can be fixed to some reasonable multiple of the scale parameter (here we set $r_0 = 4\ell$).  
The examples in Figure \ref{fig:matern} demonstrate that such a kernel readily produces correlated 
structure in the residual spectrum that is similar to those seen in a typical data--model 
comparison.  

\begin{figure*}[!htb]
\begin{center}
\includegraphics[width=0.4\textwidth]{figs/matern_matrix.pdf}
\includegraphics[width=0.4\textwidth]{figs/matern_draw.pdf}
\caption{\textbf{Left} Inset zoomed to show a small region of a typical covariance matrix, generated using the kernel in Equation~\ref{eqn:global} and common values for the hyperparameters. There is a small degree of covariance in elements within a few pixels off the diagonal, which quickly tapers off so that the majority of the matrix remains sparse ($\sigma_{ij} = 0$). This matrix is used to model a spectrum which is correlated on a $\sim$few pixel scale.
\textbf{Right} To demonstrate that this matrix properly models the correlated structure of the residuals, we compare the residuals to random residuals generated from a multivariate normal distribution with this covariance matrix. The top panel shows the same residuals shown in Figure~\ref{fig:class0}, and below are plotted three sets of simulated residuals. The amplitude and correlation length of the simulated residuals closely approximates the structure of the actual pixel residuals.}
\label{fig:matern}
\end{center}
\end{figure*}


\subsubsection{Local Covariance Structure} \label{subsec:local_covariance}

Aside from the global covariance structure described above, there are likely also local regions of 
strong, highly correlated residuals that need to be treated in the modeling framework.  These large 
amplitude residual regions are usually produced by imperfect spectral lines in the models (e.g., 
missing opacity sources, uncertain oscillator strengths, etc.); some representative examples are 
highlighted in Figure \ref{fig:badlines}.  To parameterize such regions in the covariance matrix, 
we introduce a sequence of non-stationary kernels that explicitly depend on the actual wavelength 
values of a pair of pixels (on $\lambda_i$ and $\lambda_j$), and not simply the distance between 
them ($r_{ij}$).  

\begin{figure*}[!htb]
\begin{center}
  \includegraphics{figs/badlines.pdf}
  \caption{A collection of spectral lines which have imperfect model fits.
    From left to right: \textbf{Class 0} The majority of spectral lines
    ($\gtrsim 60$\%) will have minor differences in strength between data and
    model spectrum, which produce low-amplitude correlations in the residuals
    on the length scale of the width of a typical spectral line.  \textbf{Class
    I}: Sometimes ($\lesssim 5$\% of all lines), a missing opacity source in
    the model (in this case a line-blended \ion{Ca}{2}) leaves a large, highly correlated
    patch of negative residuals.  \textbf{Class II}: Sometimes ($\lesssim 5$\%
    of all lines), an extraneous line in the model leaves a large, highly
    correlated patch of positive residuals.  \textbf{Class III}: If the line strengths are
    substantially discrepant ($\lesssim 10$\% of all lines), there will be many
    correlated residuals of moderate amplitude.  The difficulty
    with class III lines is that for any specific line, there might exist a
    $\vtstar$ that will fit the line, but there does not exist a $\vtstar$ that
    will properly fit \emph{all} the lines.}
\label{fig:badlines}
\end{center}
\end{figure*}

Assuming that these local residuals are produced primarily by pathological differences in the 
spectral line strength (rather than shape or center), a simple Gaussian is a reasonable residual 
model.  In that case, the $k^{\rm th}$ such local residual can be described as
\begin{equation}
\vR_{\lambda}(a_k, \mu_k, \sigma_k) = \frac{a_k}{\sqrt{2\pi}{\sigma_k}} \exp \left[ - \frac{r^2(\lambda,\mu_k)}{2\sigma_k^2} \right],
\end{equation}
with an amplitude $a_k$, a mean wavelength $\mu_k$, and a width $\sigma_k$.  Following 
Eq.~\ref{eqn:expectation}, the kernel that describes the covariance of any two pixels related to 
the $k^{\rm th}$ residual region is
\begin{equation}
  \mathcal{K}_k(\lambda_i, \lambda_j | a_k, \mu_k, \sigma_k) = 
  \frac{1}{2\pi} \left(\frac{a_k}{\sigma_k}\right)^2 \exp \left [ - \, \frac{r^2(\lambda_i, \mu_k) + r^2(\lambda_j, \mu_k)}{2 \sigma_k^2}\right ].
\label{eqn:klocal}
\end{equation}
We again taper this kernel with a Hann window (Eq.~\ref{eqn:Hann}) to ensure computational 
efficiency with a sparse covariance matrix; in this case, the truncation distance $r_0$ can be set 
to some multiple of the Gaussian width (we set $r_0 = 4\sigma_k$).  Figure \ref{fig:region} 
demonstrates how this non-stationary Gaussian kernel generates a localized region of enhanced 
variance that successfully mimics the kind of residuals produced by an inaccurate spectral line 
model.  These kernels down-weight the influence of such strong residuals in the likelihood 
calculation, mitigating any potential bias they might induce on inferences of the interesting 
parameters ($\vt$).  In essence, this is a robust, flexible, and unbiased method for (correlated) 
outlier rejection that preserves the integrity of the probabilistic framework being developed (as 
opposed to manual or threshold-based clipping or masking).  

\begin{figure*}[!htb]
\begin{center}
\includegraphics[width=0.4\textwidth]{figs/gauss_matrix.pdf}
\includegraphics[width=0.4\textwidth]{figs/gauss_draw.pdf}
\caption{\textbf{Left} A typical covariance matrix including the Gaussian line kernel (Equation~\ref{eqn:klocal}). The same global covariance shown in Figure~\ref{fig:matern} is still present with the same hyperparameters, however now there is an additional patch of high covariance corresponding to the large, Gaussian-shaped residuals. These larger elements in the covariance matrix effectively down-weight the contribution of a poorly modeled synthetic spectral line.
\textbf{Right} The same spectroscopic residuals shown in Figure~\ref{fig:badlines}, class I, shown with three random draws from the covariance matrix. The random draws can take on a range of amplitudes--positive, negative, or even flat--because they are simply random draws that are described by the covariance matrix. The wide range of possible residual amplitudes match the structure and amplitude of the pixel residuals.}
\label{fig:region}
\end{center}
\end{figure*}

\comm{These local kernels can be further modified to account for more complex residual structures.  For 
example, late-type stars with imperfectly modeled molecular bandheads may produce a complicated 
pattern of positive and negative residuals or a pronounced mismatch over a relatively large 
spectral scale.  This phenomenologically different local covariance behavior can still be treated 
in this framework, if we design an appropriate covariance kernel. Given the behavior of molecular bandheads, a \matern\ kernel tapered by the Gaussian kernel in Eq.~\ref{eqn:klocal} works well.}
\todo{We'll need to round out the Gl 51 example in Sect 3.2 before this text is finished.}


\subsubsection{Composite Covariance Matrix}

We can now compute the covariance matrix employed in the likelihood calculation 
(Eq.~\ref{eqn:lnprob}) as the linear combination of these kernels and the trivial pixel-by-pixel 
noise matrix, 
\begin{eqnarray}
\vC_{ij}(\vT_{\rm cov}) & = & b \, \delta_{ij}\sigma_i + \\ \nonumber
         &   &  w(r_{ij}|r_0 = 4\ell) \,\, \mathcal{K}_g(\lambda_i, \lambda_j|a_g, \ell) + \\ \nonumber
         &   &  \sum_k w(r_{ij}|r_0 = 4\sigma_k) \,\, \mathcal{S}_k(\lambda_i,\lambda_j | h_k) \,\, \mathcal{K}_k(\lambda_i, \lambda_j | a_k, \mu_k, \sigma_k),
\end{eqnarray}
where the covariance hyperparameters $\vT_{\rm cov} = [a_g, \ell, \{a_k, \mu_k, \sigma_k, 
h_k\}^{N_{\rm reg}}]$, $N_{\rm reg}$ is the number of local residual regions (see below for details 
on how this is determined), and $b$ is a fixed parameter that scales the pixel noise values to 
account for additional error (e.g., introduced by read noise, order extraction, and the grid 
interpolation noted in Section \ref{subsec:synthetic}; reasonable values are $b \approx 1.02$--1.10 
for well-calibrated optical spectra, and examples are provided in Section \ref{sec:examples}).


\subsection{Exploring the Posterior} \label{subsec:MCMC}

%When iteratively fitting a spectrum (see \S\ref{subsec:MCMC}), we continue to add line covariance kernels until we have covered all of the high amplitude residuals. Typically, we will add line kernels until all residuals greater than three times the amplitude of the global covariance kernel are covered. In a single order of an echelle spectrum, there may be $N$ regions of high covariance that are parameterized by several line kernels (Equation~\ref{eqn:klocal}), which we group into an aggregate parameter $\vtlines = \{\vtline{1}, \vtline{2}, \ldots, \vtline{N}\}$. Along with the global covariance parameters and Chebyshev parameters for this order, we call the collection of nuisance hyperparameters for a specific order  (e.g., order $1$) $\vtorder{1} = \{\vtcheb, \vtglobal, \vtlines\}$. Taken together, the aggregated nuisance parameters for all of the $N$ orders are stored in $\vtorders = \{\vtorder{1}, \vtorder{2}, \ldots, \vtorder{N} \}$.

% We explicitly fit for the
%hyperparameters of the covariance kernels at the same time we fit for the
%stellar parameters. While including these extra parameters does increase the
%dimensionality and complexity of our model, they do provide several advantages.
%\emph{A priori} we do not know which regions of the spectrum are improperly
%modeled. The covariance hyperparameters provide a framework to identify these
%regions iteratively and in a self-consistent manner. 

%The traditional manner of dealing with spectral mismatch is to simply mask out the regions of the spectrum which do not agree to within a certain tolerance.  Rather than arbitrarily excluding regions of the spectrum from the fit, these regions should instead be incorporated into in the fit with the appropriate weight. A model that includes covariance is far more likely than forcing the synthetic spectrum to fit perfectly, and far more flexible than arbitrarily masking regions which do not fit. The fitting procedure allows these weights to be determined self-consistently, such that lines which are slightly wrong can still bring information to bear on the stellar parameters. In fact, for some types of stars with difficult-to-model spectra, it may be possible that the lines that are off by a small amount (class 0 and III lines in Figure~\ref{fig:badlines}) actually provide the \emph{most information} about the stellar parameters, precisely because these lines are the most sensitive to stellar structure and consequentially are the most difficult to model correctly.

%Another powerful benefit of including the covariance hyperparameters is the result of their ability to quantify and account for model-data mismatch. When a data spectrum is fit with a high-quality spectral library, the pixel residuals are less likely to be correlated, and the covariance structure will have a smaller amplitude and correlation length. This means that the inference on the stellar parameters will be more precise, in the same way that high quality data allows a more precise result. However, if the systematic mismatch between the data and model is large, then the hyperparameters will be larger and the precision of the stellar parameters will naturally inflate to respect the quality of model-data fit. The benefits of using covariance hyperparameters also extend beyond the use case of fitting a single stellar spectrum. If we fit many stars with the same set of synthetic models, we can use the structure of the covariance matrix to improve the models themselves (see \S\ref{subsec:learning}).

%We use a Markov chain Monte Carlo (MCMC) algorithm coupled to a Gibbs sampler to explore the posterior distribution (Equation~\ref{eqn:lnprob}). The MCMC routine efficiently explores a high dimensional probability space using a stochastic, iterative approach. The Gibbs sampler provides a way to easily sample a large number of parameters in a simple, organized manner by sampling only a subset of parameters at any given time, but then rotating among all of the subsets of parameters. For more on MCMC and Gibbs samplers see \citet[Ch. 11]{gelman13} and the references therein. 

%In addition to the stellar parameters $\vtstar$, we have introduced several nuisance parameters for calibration and residual modeling. These nuisance parameters have a logical hierarchical structure. At the lowest level of the hierarchy, a single order of an echelle spectrum has $N$ regions of high covariance $\vtlines = \{\vtline{1}, \vtline{2}, \ldots, \vtline{N}\}$. If we wish to evaluate the probability of the parameters of a specific line (e.g., line $1$) conditional on the current values of all other lines, then we denote this by $p(\vt_{\textrm{line} = 1} | \vt_{\textrm{lines} \ne 1})$. The collection of nuisance parameters for a specific order  (e.g., order $1$) is the aggregate parameter $\vtorder{1} = \{\vtcheb, \vtglobal, \vtlines\}$. Taken together, the aggregated nuisance parameters for all of the $N$ orders are stored in $\vtorders = \{\vtorder{1}, \vtorder{2}, \ldots, \vtorder{N} \}$. Because the nuisance parameters for a single echelle order are independent from the nuisance parameters for any other echelle order, we have $p(\vt_{\textrm{order} = 1} | \vt_{\textrm{orders} \ne 1}) = p(\vt_{\textrm{order} = 1})$. 

%\begin{figure*}[!htb]
%\begin{center}
%  \includegraphics[width=5in]{figs/stellar_triangle_Kurucz.png}
%  \caption{The posterior probability function of the stellar parameters for WASP-14, an F star, as explored by the MCMC Gibbs sampler. These stellar parameters are marginalized over the Chebyshev ($\vT_{\rm cheb}$) and noise parameters ($\vT_{\rm cov}$). Figure generated with \texttt{triangle.py} \citep{foreman-mackey14}.
%}
%\label{fig:stellar_posterior}
%\end{center}
%\end{figure*}

%\begin{figure*}[!htb]
%\begin{center}
%  \includegraphics[draft, width=3in, height=3in]{figs/global_posteriors.pdf}
%  \includegraphics[draft, width=3in, height=3in]{figs/line_posteriors.pdf}
%  \caption{\textbf{Left}: The posterior probability function of the global covariance parameters, as explored by the MCMC Gibbs sampler, marginalized over the stellar and line noise parameters. \textbf{Right}: The posterior probability function of the parameters for a single line kernel, marginalized over the stellar and global noise parameters.}
%\label{fig:noise_posterior}
%\end{center}
%\end{figure*}

To explore the posterior probability distribution and make inferences on the parameters of 
interest, we employ a hierarchical Monte Carlo Markov Chain (MCMC) approach that nests the standard 
Metropolis-Hastings algorithm (for sampling the parameters $\vt$) inside the Gibbs sampler (which 
rotates through subsets of the hyper-parameters $\vT$).  

The Gibbs sampler iteratively explores this hierarchy of parameters. At each level of the hierarchy, corresponding to a different subset of nuisance parameters, we use the Metropolis-Hastings algorithm to propose a new subset of parameters and then either accept or reject them. This process of proposal and acceptance/rejection is called ``sampling.'' The Gibbs sampler rotates between sampling in different subsets of parameters, eventually sampling all of the nuisance parameters at a certain cadence. To initialize the MCMC algorithm, we make a reasonable guess for the starting parameters. We use existing spectral types in the literature for the stellar parameters, flat Chebyshev polynomials (no flux-calibration correction), and no covariance structure in the residuals (the global covariance parameters are set to zero and no line kernels are instantiated). We use superscripts to denote iterations of the MCMC algorithm. $i$ denotes the parameters from the current iteration and $i - 1$ denotes the previous iteration. The Gibbs sampler rotates among subsets of the parameters as follows 

\begin{enumerate}
  \item Sample in the stellar parameters. For each proposal of the Metropolis-Hastings algorithm, generate a model spectrum following the steps in \S\ref{subsec:postprocess}. When deciding whether or not to accept the parameters, the Gibbs sampler evaluates $p(\vtstar^i | \vtorders^{i-1})$.
  \item For each order of the echelle spectrum
    \begin{enumerate}
      \item Sample in the Chebyshev polynomial parameters. Adjust the spectrum as detailed in \S\ref{subsec:postprocess}.Gibbs sampler evaluates $p(\vtcheb^{i} | \vtstar^{i}, \vtglobal^{i -1}, \vtlines^{i-1})$.
      \item Sample in the global covariance parameters. Adjust the covariance matrix $C$ as described in \S\ref{subsec:global_covariance}. Gibbs sampler evaluates $p(\vtglobal^{i}| \vtstar^{i}, \vtcheb^{i}, \vtlines^{i -1})$.

      \item Check to see whether the algorithm should instantiate/delete new/old line kernels
      \item For each line kernel $k$
	\begin{enumerate}
	  \item Sample in the parameters for each line, conditional on the parameters for all the other lines. Adjust the covariance matrix $C$ as described in \S\ref{subsec:local_covariance}. Gibbs sampler evaluates $p(\vtline{k}^i| \vt_{\textrm{lines}\ne k}^{i - 1})$.
	\end{enumerate}
    \end{enumerate}
\end{enumerate}

At each stage of the Gibbs sampler, the likelihood function (Equation~\ref{eqn:lnprob}) is evaluated for a proposal of a particular subset of parameters, conditional on the current values of all the other parameters. The hierarchical structure of the parameters influences how the algorithm may be efficiently implemented in code. For a typical optical spectrum with $\gtrsim$1,000 pixels, the most computationally intensive step of the likelihood evaluation is usually the matrix product $\fR^T C^{-1} \fR$. Since we have designed the covariance matrix to be sparse, we can use optimized sparse matrix algorithms which are much faster and memory efficient than dense matrix operations. Because we are not interested in the matrix inverse $C^{-1}$ by itself, but rather the product with the residual vectors, we can use efficient routines for solving linear systems to bypass the computationally difficult step of matrix inversion. Additionally, because the covariance matrix is positive semi-definite, we can use the Cholesky factorization of the matrix to optimize the evaluation of the matrix product. Once the covariance matrix is factorized, any subsequent evaluation of the matrix product for different residual vectors $\fR$ is extremely rapid. This makes the $\vtstar$ and $\vtcheb$ steps of the Gibbs sampler extremely fast.
When we sample in the nuisance parameters which affect the covariance matrix, we must redo the Cholesky factorization of $C$ for each update. However, because we designed the kernels to deliver a sparse matrix these operations are still moderately efficient.  We use the high-performance \texttt{SuiteSparse/CHOLMOD}\footnote{\url{http://www.cise.ufl.edu/research/sparse/cholmod/}} library to implement the sparse matrix and Cholesky factorization operations \citep{chen08, davis09} and utilize the Metropolis-Hastings sampler included in the \texttt{Python} MCMC package \texttt{emcee} \citep{foreman-mackey12}.

We run the MCMC Gibbs sampler for many iterations until the estimate of the posterior distribution has converged. To check that the chain is not stuck in a local maximum of the posterior, we redo the MCMC run many times with different starting parameters, to ensure that the algorithm converges to the same global maximum. A major advantage of using the MCMC algorithm to explore the multidimensional probability space is that it provides numerical samples in each dimension. Therefore, marginalizing out a parameter (i.e., numerically integrating over a dimension in probability space) is as simple as combining all of the samples in this dimension. This enables us to present a posterior of the stellar parameters $\vtstar$ (Figure~\ref{fig:stellar_posterior}) which has been marginalized over all of the nuisance hyperparameters. This posterior is the final estimate of the stellar parameters which \emph{incorporates} any inherent uncertainty due to model mismatch (via the covariance hyperparameters) and flux-calibration (via the Chebyshev polynomials).

The benefit of including nuisance parameters and then marginalizing over them is that we can self-consistently model the uncertainty inherent to any spectrum while naturally capturing any degeneracy between the model parameters. If stellar parameters are estimated using a method which ignores these nuisance parameters (e.g., by-eye fitting) and the astronomer arbitrarily inflates the parameter uncertainties to reflect intuition, the degeneracy between parameters is artificially destroyed. 

\subsection{Applications}
\label{subsec:learning}
By cataloguing the covariance structure of the residuals, especially those generated from strong spectral line mismatch, we collect valuable information about the quality of the synthetic spectra. After fitting many stars, the accumulated knowledge of the data-model mismatch can be used when fitting a new star. The previous structure of the covariance matrix allows us to set priors on what the covariance in certain regions of the spectrum should be, which will speed convergence for this new star. Additionally, after fitting several stars, the average value of the covariance matrix will inform us about the quality of specific spectral lines in the synthetic models. 

Linking the covariance matrices of stars could be done serially, where the aggregate covariance matrix of all previous fits is used as a prior for the current iteration. Or, the covariance matrices could instead be linked hierarchically, in that the parameters describing the depth and width of a line residual for a specific star $a_1$ and $\sigma_1$, are modeled as coming from of a population of possible depths and widths for a given spectral type. Each stellar spectrum will have a slightly different realization of a spectral line, which will have some scatter about the average residual height. Linking the covariance matrix between spectra of similar stars allows us to grow more confident in our assessment that certain synthetic spectral lines are indeed outliers and should be appropriately down-weighted. In turn, as we become more certain of the weights, the stellar posteriors will become narrower and make our estimates of the stellar parameters more precise. This ability of the model to mutually inform sets of parameters is one of the major advantages of hierarchical Bayesian analysis \citep{kruschke10}.

Once determined, this average covariance matrix could be delivered to the communities that created the synthetic libraries, which would enable them to rapidly pinpoint and correct non-physical lines in the synthetic models. Alternatively, we could correct the models ourselves by using the chain of logic and mathematical post-processing that we used to created the synthetic model spectrum to reverse-engineer what the behavior of the raw synthetic spectrum \emph{should} be, at the raw $R \gtrsim$~100,000 resolution. This fundamental application of machine learning would enable us to create our own library of data driven, semi-empirical stellar models. Rather than simply assembling an empirical spectral library using only real stellar spectra, this combined approach is more powerful because the synthetic stellar atmospheres provide an actual anchor point of fundamental stellar parameters tied down by the laws of stellar physics.

\section{Examples}
\label{sec:examples}

\subsection{Fitting data}

To demonstrate our technique, we fit spectra of two different stars with varying stellar properties and acquired with different instruments. The first object is WASP-14, an F5 star \citep{joshi09}. The spectrum was obtained with the Tillinghast Reflector Echelle Spectrograph (\emph{TRES}) \citep{mink11} on the FLWO 1.5m telescope on June 14th, 2009. \emph{TRES} delivers a 51 echelle spectrum spanning the full optical wavelength spectrum $3860 - 9100$ \AA\ at a resolution of $R \sim$ 44,000. \citet{torres12} acquired the spectrum of WASP-14 to study the stellar properties of transiting exoplanet hosts. The raw spectrum was reduced using the well-tested \emph{TRES} pipeline written by L. Buchhave. In the \ion{Mg}{0}b order ($\sim 5100$ \AA), the spectrum has a signal to noise of 150 per resolution element.

The SPC analysis within \citet{torres12} focused on a region of spectrum from $5000 5300$ \AA and used many methods. The other methods are not directly comparable because they use different underlying assumptions. Additionally, there are systematics introduced by metallicity assumptions and radiative transfer. Using this same spectral library as SPC, we recover the same parameters.

When we fit this spectrum using the same assumptions, (amounting to a $\delta$-function prior on $\log g$. global noise amplitude is so low, almost non-existent. This demonstrates sensitivity to a prior as well as the fact that surface gravity estimates from spectroscopy may be biased against fits derived from planetary transits.

The second object is Gl 51, an M5 star. The spectrum was obtained with the \emph{SPEX} instrument at IRTF. The spectrum is at moderate resolution ($R \sim $ 2,000), mid-infrared spectrum from $0.7 - 5.5\mu$m. Gl 51 is a M5 star from the IRTF spectral library \citep{cushing05, rayner09} taken with the SPEX instrument \citep{rayner03}. \citep{rojas-ayala12} use the H20 -K index (defined over what wl range?). 

A Bayesian approach allows you to explore sensitivity to prior assumptions.

\subsection{Generic Tests}

Designed to show off what the model framework can actually do. 

\begin{figure*}[!htb]
\begin{center}
  \includegraphics[draft,width=0.9\textwidth,height=4in]{figs/spectrum_region.pdf}
  \caption{The spectrum residuals and many random draws from the final covariance matrix showing how the uncertainty in the ``outlier'' lines is identified by the local covariance kernels.}
\label{fig:regions}
\end{center}
\end{figure*}

Talk about giving more reasonable errors using regions, in the biased case.
Show a figure of the regions determined, plotted with the increased variance envelope over the residuals.

\begin{itemize}
  \item Spectral parameters can vary by a large margin depending on which spectral library you use ($200~K$ or more).
  \item Both spectral libraries have stars that they perform better and worse on.
  \item This will be reflected in the increased level of global noise, and number of ``bad'' regions that have been instantiated.
\end{itemize}

Summary end of this section is that we are

1) validating other techniques (WASP14, Kurucz, etc)
2) and two different synthetic libraries give different answers

this can feed into a discussion about tweaking models.


\section{Discussion}
\label{sec:discussion}

We can (should?) add an additional level to the hierarchy of
  hyperparameters and add parameters to describe the \emph{population
  characteristics} of poorly modeled spectral lines (mostly the typical width,
  amplitude of these lines).  This will tell us about the frequency and
distribution of spectral modelling errors.

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{yahapj}
\bibliography{stellarspectra}

\section{Tables}

\begin{deluxetable}{ll}
\tablecaption{Nomenclature used in this document}
\tablehead{
  \colhead{Symbol} & \colhead{Description}
}
\startdata
$i$ & index specifying a pixel\\
$\lambda_i$ & wavelength corresponding to a given pixel $i$\\
$\vg$ & fundamental stellar parameters, $T_{\rm eff}, \log(g), \Z, \A$\\
  & that parameterize a synthetic spectrum from the grid\\
$\vpp$ & stellar parameters $v \sin i$, $v_z$, $A_V$, and $R^2/d^2$ that\\
  & are applied during ``post processing'' of the synthetic spectrum\\
$\vtstar$ & $\{\vg,\vpp \}$\\
$\finst(\lambda)$ & data spectrum\\
$\fsynth(\lambda)$ & synthetic spectrum\\
$\vtcheb$ & the set of Chebyshev polynomial coefficients $\{c_0, c_1, \ldots, c_N\}$\\
$\vtline{1}$ & \\
$\vtlines$ & \\
$\vtorder{1}$ & \\
$\vtorders$ & \\
$\vt$ & the parameters $\{\vg, \vpp, \vN\}$ that completely describe a model spectrum\\
$\fDi$ & data flux for a given pixel, $D(\lambda_i)$\\
$\fD$ & data vector comprised of all $\fDi$, $i = \{1, \ldots, N\}$\\
$\fMi$ & model flux for a given pixel, $M(\lambda_i | \vt)$\\
$\fM$ & model vector comprised of all $\fMi$, $i = \{1, \ldots, N\}$\\
$\sigma_i$ & Poisson noise for a given pixel $i$\\
$R_i$ & residuals $\fDi - \fMi$\\
$\fR$ & residual vector $\fD - \fM$\\
$C$ & covariance matrix\\
$\sigma_{ij}$ & element in the covariance matrix\\
$r(\lambda_i, \lambda_j)$ & radial distance in wavelength space corresponding to $\Delta v$\\
$k_{\rm global}$ & global covariance kernel\\
$k_{\rm line}$ & regional covariance kernel\\
\enddata
\end{deluxetable}

\begin{deluxetable}{llrlr@{$\pm$}lr@{$\pm$}lr@{$\pm$}ll} 
\tablecaption{\label{table:tests} Tests}
\tablehead{
  \colhead{Object} & \colhead{Orders} & \colhead{$\lambda$ range [\AA]} & \colhead{library} &
  \multicolumn{2}{c}{$T_{\rm eff} \pm \sigma$} & \multicolumn{2}{c}{$\log g \pm \sigma$} &
  \multicolumn{2}{c}{$\Z \pm \sigma$} & \colhead{comments}
}
\startdata
WASP-14 & 22-24 & 5060-5315 & Kurucz & 6310 & 52 & 3.91  & 0.08 & -0.38  & 0.03 & Poisson only\\
WASP-14 & 22-24 & 5060-5315& Kurucz & 6314 & 50 & 3.92 & 0.08 & -0.39 & 0.03 & Matern only\\
WASP-14 & 22-24 & 5060-5315& Kurucz & 6512 & 32 & \multicolumn{2}{c}{\nodata} & -0.27 & 0.02 & Matern + fixed $\log g = 4.29$\\
WASP-14 & 23 & & PHOENIX & 6021 & 16 & 3.83 & 0.03 & -0.50 & 0.01 & Poisson only \\
WASP-14 & 23 & & PHOENIX & 5965 & 70 & 3.78 & 0.09 & -0.68 & 0.07 & Matern only, no regions (conv?) \\
WASP-14 & 23 & & PHOENIX & 5883 & 55 & 3.71 & 0.07 & -0.74 & 0.05 & Matern and regions (conv?) \\
WASP-14 & 22-24 & 5060-5315 & PHOENIX & 6117 & 30 & 3.73 & 0.06 & -0.52 & 0.02 & Poisson only \\
WASP-14 & 22-24 & 5060-5315 & PHOENIX & 5865 & 43 & 3.2 & 0.08 & -0.85 & 0.03 & Matern only. Because of 24. \\
WASP-14 & 22-24 & 5060-5315 & PHOENIX & & & \multicolumn{2}{c}{\nodata} & & & Matern + fixed $\log g = 4.29$ \\
WASP-14 & 22-24 & 5060-5315 & PHOENIX & & & & & & & Regions included \\
\enddata
\end{deluxetable}

\appendix


This version of the paper was generated
 from a git repository available at \url{http://github.com/iancze/StellarSpectra/}
 with git hash \texttt{\githash\,(\gitdate)}.
\end{document}
